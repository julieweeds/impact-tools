Aligning Packed Dependency Trees: a theory
of composition for distributional semantics
David Weir∗
Jeremy Refﬁn∗

Julie Weeds∗
Thomas Kober∗

University of Sussex, UK

University of Sussex, UK

University of Sussex, UK

University of Sussex, UK

We present a new framework for compositional distributional semantics in which the distribu-
tional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show
that these structures have the potential to capture the full sentential contexts of a lexeme and
provide a uniform basis for the composition of distributional knowledge in a way that captures
both mutual disambiguation and generalization.

1. Introduction

This paper addresses a central unresolved issue in distributional semantics: how to
model semantic composition. Although there has recently been considerable interest in
this problem, it remains unclear what distributional composition actually means. Our
view is that distributional composition is a matter of contextualizing the lexemes being
composed. This goes well beyond traditional word sense disambiguation, where each
lexeme is assigned one of a ﬁxed number of senses. Our proposal is that composition
involves deriving a ﬁne-grained characterization of the distributional meaning of each
lexeme in the phrase, where the meaning that is associated with each lexeme is bespoke
to that particular context.

Distributional composition is, therefore, a matter of integrating the meaning of each
of the lexemes in the phrase. To achieve this we need a structure within which all of the
lexemes’ semantics can be overlaid. Once this is done, the lexemes can collectively agree
on the semantics of the phrase, and in so doing, determine the semantics that they have
in the context of that phrase. Our process of composition thus creates a single structure
that encodes contextualized representations of every lexeme in the phrase.

The (uncontextualized) distributional knowledge of a lexeme is typically formed
by aggregating distributional features across all uses of the lexeme found within the
corpus, where distributional features arise from co-occurrences found in the corpus. The
distributional features of a lexeme are associated with weights that encode the strength
of that feature. Contextualization involves inferring adjustments to these weights to
reﬂect the context in which the lexeme is being used. The weights of distributional
features that don’t ﬁt the context are reduced, while the weight of those features that
are compatible with the context can be boosted.

∗ Department of Informatics, University of Sussex. Falmer, Brighton BN1 9QH, UK. E-mail:

d.j.weir@sussex.ac.uk, j.e.weeds@sussex.ac.uk, j.p.refﬁn@sussex.ac.uk, t.kober@sussex.ac.uk

© 2006 Association for Computational Linguistics

Computational Linguistics

Volume 1, Number 1

As an example, consider how we contextualize the distributional features of the
word wooden in the context of the phrase wooden ﬂoor. The uncontextualized represen-
tation of wooden presumably includes distributional features associated with different
uses, for example The director ﬁred the wooden actor and I sat on the wooden chair. So,
while we may have observed in a corpus that it is plausible for the adjective wooden to
modify ﬂoor, table, toy, actor and voice, in the speciﬁc context of the phrase wooden ﬂoor,
we need to ﬁnd a way to down-weight the distributional features of being something
that can modify actor and voice, while up-weighting the distributional features of being
something that can modify table and toy.

In the example above we considered so-called ﬁrst-order distributional features;
these involve a single dependency relation, e.g. an adjective modifying a noun. Similar
inferences can also be made with respect to distributional features that involve higher-
order grammatical dependencies1. For example, suppose that we have observed that a
noun that wooden modiﬁes (e.g. actor) can be the direct object of the verb ﬁred, as in The
director ﬁred the wooden actor. We want this distributional feature of wooden to be down-
weighted in the distributional representation of wooden in the context of wooden table,
since things made of wood do not typically lose their job.

In addition to specialising the distributional representation of wood to reﬂect the
context wooden ﬂoor, the distributional representation of ﬂoor should also be reﬁned,
down-weighting distributional features arising in contexts such as Prices fell through the
ﬂoor, while up-weighting distributional features arising in contexts such as I polished the
concrete ﬂoor.

In our example, some of the distributional features of wooden, in particular, those
to do with the noun that this sense of wooden could modify, are internal to the phrase
wooden ﬂoor in the sense that they are alternatives to one of the words in the phrase.
Although it is speciﬁcally a ﬂoor that is wooden, our proposal is that the contextualized
representation of wooden should recognise that it is plausible that nouns such as chair
and toy could be modiﬁed by the particular sense of wooden that is being used. The
remaining distributional features are external to the phrase. For example, the verb mop
could be an external feature, since things that can be modiﬁed by wooden can be the
direct object of mop. The external features of wooden and ﬂoor with respect to the phrase
wooden ﬂoor provide something akin to the traditional interpretation of the distributional
semantics of the phrase, i.e. a representation of those (external) contexts in which this
phrase can occur.

While internal features are, in a sense, inconsistent with the speciﬁc semantics of
the phrase, they provide a way to embellish the characterization of the distributional
meaning of the lexemes in the phrase. Recall that our goal is to infer a rich and ﬁne-
grained representation of the contextualized distributional meaning of each of the
lexemes in the phrase.

Having introduced the proposal that distributional composition should be viewed
as a matter of contextualization, the question arises as to how to realise this conception.
Since each lexeme in the phrase needs to be able to contribute to the contextualization of
the other lexemes in the phrase, we need to be able to align what we know about each of
the lexeme’s distributional features so that this can be achieved. The problem is that the
uncontextualized distributional knowledge associated with the different lexemes in the
phrase take a different perspective on the feature space. To overcome this we need to:

1 Given some dependency tree, a k-th order dependency holds between two lexemes (nodes) in the tree

when the path between the two lexemes has length k.

2

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

(a) provide a way of structuring the distributional feature space, which we do by typing
distributional features with dependency paths; and (b) ﬁnd a way to systematically
modify the perspective that each lexeme has on this structured feature space in such
a way that they are all aligned with one another.

Following Baroni and Lenci (2010), we use typed dependency relations as the
bases for our distributional features, and following Padó and Lapata (2007), we include
higher-order dependency relations in this space. However, in contrast to previous pro-
posals, in our model, the higher order dependency relations provides structure to the
space which is crucial to our deﬁnition of composition. Each co-occurrence associated
with a lexeme such as wooden is typed by the path in the dependency tree that connects
the lexeme wooden with the co-occurring lexeme, e.g. ﬁred. This allows us to encode a lex-
eme’s distributional knowledge with a hierarchical structure that we call an Anchored
Packed Dependency Tree (APT). As we show, this data structure provides a way for us
to align the distributional knowledge of the lexemes that are being composed in such a
way that the inferences needed to achieve contextualization can be implemented.

2. The Distributional Lexicon

In this section, we begin the formalisation of our proposal by describing the distribu-
tional lexicon: a collection of entries that characterize the distributional semantics of
lexemes. Figure 1 provides a summary of the notation that we are using.

Let V be a ﬁnite alphabet of lexemes, where each lexeme is assumed to incorporate a
part-of-speech tag;2let R be a ﬁnite alphabet of grammatical dependency relations; and
let TV,R be the set of dependency trees where every node is labeled with a member of V ,
and every directed edge is labeled with an element of R. Figure 1 shows eight examples
of dependency trees.

2.1 Typed Co-occurrences

type of this co-occurrence, capturing the syntactic relationship that holds between these
occurrences of the two lexemes. In particular, τ encodes the sequence of dependencies

When two lexemes w and w′ co-occur in a dependency tree3 in t∈ TV,R, we represent this
co-occurrence as a triplew, τ, w′ where τ is a string that encodes the co-occurrence
that lie along the path in t between the occurrences of w and w′ in t. In general, a path
from w to w′ in t initially travels up towards the root of t (against the directionality of
the dependency edges) until an ancestor of w′ is reached. It then travels down the tree to
w′ (following the directionality of the dependencies). The string τ must, therefore, not
the pathv0, . . . , vk in t, where k> 0, w labels v0 and w′ labels vk, the string τ= x1 . . . xk
r

only encode the sequence of dependency relations appearing along the path, but also
whether each edge is traversed in a forward or backward direction. In particular, given

encodes the co-occurrence type associated with this path as follows:

if the edge connecting vi−1 and vi runs from vi−1 to vi and is labeled by r
then xi= r; and

2 There is no reason why lexemes could not include multi-word phrases tagged with an appropriate part of

speech.

3 In order to avoid over-complicating our presentation, when possible, we do not distinguish between a

node in a dependency tree and the lexeme that appears at that node.

3

Computational Linguistics

Volume 1, Number 1

Notation

Description

V
w
R
r
R
r
x
t

TV,R

τ

C

A

τ−1
w, τ, w′
↓(τ)
#(w, τ, w′, t)
#w, τ, w′
w
w(τ, w′)
w(τ)
 τ, w
W(w, τ, w′)
——→w
SIM(w1,w2)
wδ
t ort
w; t
ࣶ{ A1, . . . , An}

FEATS

a ﬁnite set of lexemes
a lexeme
a ﬁnite set of dependency tree edge labels
an element of R
a ﬁnite set of inverse dependency tree edge labels
an element of R

an element of R∪ R

a dependency tree
the dependency trees over lexemes V and dependencies R
a co-occurrence type (path)
the inverse (reverse) of path τ

the (uncontextualized) APT for w
an APT

a corpus of (observed) dependency trees
the co-occurrence type produced by reducing τ

the co-occurrence of w with w′ with co-occurrence type τ
number of occurrences ofw, τ, w′ in t
number of occurrences ofw, τ, w′ in the corpus
the weight for w′ inw at node for co-occurrence type τ
the node (weighted lexeme multiset) inw for co-occurrence type τ
the weight of the distributional feature τ, w′ of lexeme w
the vector representation of the APTw
the distributional similarity ofw1 andw2
the APTw that has been offset by δ
the result of merging aligned APTs in{ A1, . . . , An}

the set of all distributional features arising in C
a distributional feature in vector space

the composed APT for the tree t
the APT for wi when contextualized by t

Table 1
Summary of notation

r

4

if the edge connecting vi−1 and vi runs from vi to vi−1 and is labeled by r
then xi= r.

∗

R∗, where R={ r  r∈ R}.

Hence, co-occurrence types are strings in R

It is useful to be able to refer to the order of a co-occurrence type, where this simply
refers to the length of the dependency path. It is also convenient to be able to refer

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

to the inverse of a co-occurrence type. This can be thought of as the same path, but

traversed in the reverse direction. To be precise, given the co-occurrence type τ= x1⋅ . . .⋅
R∗ for 1≤ i≤ n, the inverse of τ, denoted τ−1, is the path xn
−1⋅ . . .⋅
xn where each xi∈ R
∗
−1= r for r∈ R. For example, the inverse of AMOD⋅DOBJ⋅NSUBJ is
−1 where r−1= r and r
NSUBJ⋅DOBJ⋅AMOD.

The following typed co-occurrences for the lexeme white/JJ arise in the tree shown

x1

in Figure 1(a).

white~JJ, AMOD⋅AMOD, ﬁzzy~JJ
white~JJ, AMOD⋅AMOD, dry~JJ
white~JJ, , white~JJ
white~JJ, AMOD, wine~NN

white~JJ, AMOD⋅DOBJ⋅NSUBJ, we~PRP
white~JJ, AMOD⋅DOBJ, bought~VBD
white~JJ, AMOD⋅DET, the~DT
white~JJ, AMOD⋅AMOD⋅ADVMOD, slightly~RB
Notice that we have included the co-occurrence white~JJ, , white~JJ. This gives
along the same dependency, e.g. in the co-occurrence white~JJ, AMOD⋅AMOD, dry~JJ,
it is logical to consider white~JJ, AMOD⋅DOBJ⋅DOBJ⋅AMOD, dry~JJ a valid co-occurrence.
in line with our decision to include white~JJ, , white~JJ rather than
white~JJ, AMOD⋅AMOD, white~JJ, all co-occurrence types are canonicalized through a de-
cancelled out. In particular, all occurrences within the string of either rr or rr for r∈ R
The reduced co-occurrence type produced from τ is denoted↓(τ), and deﬁned as

a uniformity to our typing system that simpliﬁes the formulation of distributional
composition in Section 4, and leads to the need for a reﬁnement to our co-occurrence
type encodings. Since we permit paths that traverse both forwards and backwards

pendency cancellation process in which adjacent, complementary dependencies are

are replaced with , and this process is repeated until no further reductions are possible.

However,

follows:

↓(τ)=↓(τ1τ2)

τ

if τ= τ1 r r τ2 or τ= τ1 r r τ2 for some r∈ R

otherwise

(1)

For the remainder of the paper, we only consider reduced co-occurrence types when
associating a type with a co-occurrence.

Given a tree t∈ TV,R, lexemes w and w′ and reduced co-occurrence type τ, the
number of times that the co-occurrencew, τ, w′ occurs in t is denoted #(w, τ, w′, t),
and, given some corpus C of dependency trees, the sum of all #(w, τ, w′, t) across all
t∈ C is denoted #w, τ, w′. Note that in order to simplify our notation, the dependence

on the corpus C is not expressed in our notation.

It is common to use alternatives to raw counts in order to capture the strength
of each distributional feature. A variety of alternatives are considered during the ex-
perimental work presented in Section 5. Among the options we have considered are
probabilities and various versions of positive pointwise mutual information. While, in
practice, the precise method for weighting features is of practical importance, it is not
an intrinsic part of the theory that this paper is introducing. In the exposition below

we denote the weight of the distributional feature  τ, w′ of the lexeme w with the
expression W(w, τ, w′).

5

Computational Linguistics

Volume 1, Number 1

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

DOBJ

DET

AMOD

AMOD

NSUBJ

ADVMOD

AMOD

we/PRP bought/VBD the/DT slightly/RB ﬁzzy/JJ dry/JJ white/JJ wine/NN

POSS

AMOD

NSUBJ

DOBJ

your/PRP$ dry/JJ joke/NN caused/VBD laughter/NN

nsubj

DOBJ

DET

AMOD

AMOD

he/PRP folded/VBD the/DT clean/JJ dry/JJ clothes/NNS

POSS

NSUBJ

XCOMP

your/PRP$ clothes/NNS look/VBP great/JJ

DET

NSUBJ

PRT

DOBJ

DET

AMOD

the/DT man/PRP hung/VBD up/RP the/DT wet/JJ clothes/NNS

TMOD

DOBJ

DET

DET

NSUBJ

ADVMOD

AMOD

a/DT boy/PRP bought/VBD some/DT very/RB expensive/JJ clothes/NNS yesterday/NN

DOBJ

NSUBJ

PRT

NMOD

CASE

DET

she/PRP folded/VBD up/RP all/DT of/IN the/DT laundry/NNS

NMOD

NSUBJ

CASE

he/PRP folded/VBD under/IN pressure/NN

Figure 1
A small corpus of dependency trees.

6

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

2.2 Anchored Packed Trees

Given a dependency tree corpus C⊂ TV,R and a lexeme w∈ V , we are interested in

(2)

capturing the aggregation of all distributional context of w in C within a single structure.
We achieve this with what we call an Anchored Packed Tree (APT). APTs are central to
the proposals in this paper: not only can they be used to encode the aggregate of all
distributional features of a lexeme over a corpus of dependency trees, but they can also
be used to express the distributional features of a lexeme that has been contextualized
within some dependency tree (see Section 4).

The APT for w given C, is denotedw, and referred to as the elementary APT for w.
Below we describe a tree-based interpretation ofw, but in the ﬁrst instance we deﬁne
R∗ and w′∈ V , such thatw(τ, w′) gives
it as a mapping from pairs(τ, w′) where τ∈ R
∗
the weight of the typed co-occurrencew, τ, w′ in the corpus C. It is nothing more
R∗ and w′∈ V :
features of w. In other words, for each τ∈ R
∗
′)

than those components of the weight function that specify the weights of distributional

k-th order APT. The distributional lexicon derived from a corpus C is a collection of

The restriction ofw to co-occurrence types that are at most order k is referred to as a
lexical entries where the entry for the lexeme w is the elementary APTw.
w(τ) is thought of as a node that is associated with the weighted lexeme multiset in
which the weight of w′ in the multiset isw(τ, w′). We refer to the nodew() as the
anchor of the APTw.

Formulating APTs as functions simpliﬁes the deﬁnitions that appear below. How-
ever, since an APT encodes co-occurrences that are aggregated over a set of dependency
trees, they can also be interpreted as having a tree structure. In our tree-based interpre-
tation of APTs, nodes are associated with weighted multisets of lexemes. In particular,

′)= W(w, τ, w

w(τ, w

Figure 2 shows three elementary APTs that can be produced from the corpus shown
in Figure 1. On the far left we give the letter corresponding to the sentence in Figure 1
that generated the typed co-occurrences. Each column corresponds to one node in the
APT, giving the multiset of lexemes at that node. Weights are not shown, and only non-
empty nodes are displayed.

It is worth dwelling on the contents of the anchor node of the top APT in Fig-
ure 2, which is the elementary APT for dry/JJ. The weighted multiset at this node is

denotedw(), and at this node, dry/JJ occurs three times. Thus, the weightw(, dry~JJ)
in trees in Figure 1: dry~JJ, AMOD⋅AMOD, ﬁzzy~JJ, dry~JJ, AMOD⋅AMOD, white~JJ and
dry~JJ, AMOD⋅AMOD, clean~JJ, all of which involve the co-occurrence type AMOD⋅AMOD.
These lexemes appear in the multisetw() because↓(AMOD⋅AMOD)= .

reﬂects this count in some way. Three other lexemes also occur at this same node:
ﬁzzy/JJ, white/JJ and clean/JJ. These lexemes arose from the following co-occurrences

3. APT Similarity

One of the most fundamental aspects of any treatment of distributional semantics is that
it supports a way of measuring distributional similarity. In this section, we describe a
straightforward way in which the similarity of two APTs can be measured through a
mapping from APTs to vectors.

7

Computational Linguistics

Volume 1, Number 1

anchor

DOBJ

POSS

DET

NSUBJ

ADVMOD

AMOD

NSUBJ

DOBJ

(a) we bought

the slightly ﬁzzy wine

⋮
⋮
⋮
⋮
⋮

your

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮
⋮

⋮
⋮
⋮

dry
white
dry
joke
clean clothes
dry

⋮
⋮
⋮
⋮
⋮

(b)
(c) he

folded

the

caused laughter

⋮
⋮
⋮
⋮
⋮

anchor

TMOD

DOBJ

POSS

DET

⋮
⋮
⋮
⋮

⋮
⋮
⋮

⋮
⋮
⋮
⋮

⋮
⋮

(c)

he

folded

DET

NSUBJ

PRP

ADVMOD

AMOD

NSUBJ

XCOMP

⋮
⋮

the

⋮
⋮

⋮
⋮
⋮
⋮
⋮ your
⋮
⋮
⋮

⋮
⋮
⋮
⋮

clothes

⋮

clean
dry

⋮

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

yesterday

(d)
(e) the man hung up
(f)

boy bought

a

clothes look great
the
clothes
some very expensive clothes

wet

anchor

DOBJ

NMOD

PRP

CASE

⋮

NSUBJ

(c) he folded ⋮
⋮
⋮
⋮
⋮
(h) he folded ⋮ under pressure

(g) she folded up

⋮
⋮
⋮

⋮

the clean clothes

DET

AMOD

⋮
⋮
⋮

dry

⋮
⋮

⋮
⋮

NMOD

CASE

DET

⋮
⋮
⋮

⋮
⋮
⋮

⋮
⋮
⋮

all

of the laundry

Figure 2
The distributional lexicon produced from the trees in Figure 1 with the elementary APT for dry/JJ
at the top, the elementary APT for clothes/NNS in the middle, and the elementary APT for
folded/VBD at the bottom. Part of speech tags and weights have been omitted.

First deﬁne the set of distributional features

FEATS= τ, w

′ w′∈ V , τ∈ R

∗

R∗ and W(w, τ, w′)> 0 for some w∈ V 

(3)

8

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

——→w[ τ, w

The vector space that we use to encode APTs includes one dimension for each element

of FEATS, and we use the pair τ, w to refer to its corresponding dimension.
Given an APT A, we denote the vectorized representation of A with—→A, and the
value that the vector —→A has on dimension  τ, w′ is denoted —→A[ τ, w′]. For each
 τ, w′∈ FEATS:
where φ(τ, w) is a path weighting function which is intended to reﬂect the fact that not
syntactically closer. By multiplying each W(w, τ, w′) by φ(τ, w) we are able capture
this give a suitable instantiation of φ(τ, w).
One option for φ(τ, w) is to use p(τ  w), i.e. the probability that when randomly
selecting one of the co-occurrencesw, τ′, w′, where w′ can be any lexeme in V , τ′ is the

all of the distributional features are equally important in determining the distributional
similarity of two APTs. Generally speaking, syntactically distant co-occurrences provide
a weaker characterization of the semantics of a lexeme than co-occurrences that are

′]= φ(τ, w) W(w, τ, w

co-occurrence type τ. We can estimate these path probabilities from the co-occurrence
counts in C as follows:

′)

p(τ  w)= #w, τ,∗
#w,∗,∗
#w, τ,∗= ∑w′∈V #w, τ, w′
#w,∗,∗= ∑w′∈V∑τ∈ ¯R∗R∗ #w, τ, w′

(4)

(5)

(6)

where

r
r

p(τ  w) typically falls off rapidly as a function of the length of τ as desired.
The similarity of two APTs, A1 and A2, which we denote SIM(A1, A2), can be
measured in terms of the similarity of vectors—→A1 and—→A2. The similarity of vectors can

be measured in a variety of ways (Lin 1998a; Lee 1999; Weeds and Weir 2005; Curran
2004). One popular option involves the use of the cosine measure:

SIM(A1, A2)= cos(—→A1,—→A2)

It is common to apply cosine to vectors containing positive pointwise mutual informa-
tion (PPMI) values. If the weights used in the APTs are counts or probabilities then they
can be transformed into PPMI values at this point.

As a consequence of the fact that the different co-occurrence types of the co-
occurrences associated with a lexeme are being differentiated, vectorized APTs are
much sparser than traditional vector representations used to model distributional se-
mantics. This can be mitigated in various ways, including:

reducing the granularity of the dependency relations and/or the
part-of-speech tag-set;
applying various normalizations of lexemes such as case normalization,
lemmatization, or stemming;

9

r
r

Computational Linguistics

Volume 1, Number 1

disregarding all distributional features involving co-occurrence types over
a certain length;
applying some form of distributional smoothing, where distributional
features of a lexeme are inferred based on the features of distributionally
similar lexemes.

4. Distributional Composition

In this section we turn to the central topic of the paper, namely distributional composi-
tion. We begin with an informal explanation of our approach, and then present a more
precise formalisation.

4.1 Discussion of Approach

Our starting point is the observation that although we have shown that all of the
elementary APTs in the distributional lexicon can be placed in the same vector space
(see Section 3), there is an important sense in which APTs for different parts of speech
are not comparable. For example, many of the dimensions that make sense for verbs,
such as those involving a co-occurrence type that begins with DOBJ or NSUBJ, do not make
sense for a noun. However, as we now explain, the co-occurrence type structure present
in an APT allows us to address this, making way for our deﬁnition of distributional
composition.

Consider the APT for the lexeme dry/JJ shown at the top of Figure 2. The anchor of
this APT is the node at which the lexeme dry/JJ appears. We can, however, take a different
perspective on this APT, for example, one in which the anchor is the node at which the
lexemes bought/VBD and folded/VBD appear. This APT is shown at the top of Figure 3.
Adjusting the position of the anchor is signiﬁcant because the starting point of the paths
given by the co-occurrence types changes. For example, when the APT shown at the top
of Figure 3 is applied to the co-occurrence type NSUBJ, we reach the node at which the
lexemes we/PRP and he/PRP appear. Thus, this APT can be seen as a characterisation of
the distributional properties of the verbs that nouns that dry/JJ modiﬁes can take as their
direct object. In fact, it looks rather like the elementary APT for some verb. The lower
tree in Figure 3 shows the elementary APT for clothes/NNS (the centre APT shown in
Figure 2) where the anchor has been moved to the node at which the lexemes folded/VBD,
hung/VBD and bought/VBD appear.

Notice that in both of the APTs shown in Figure 3 parts of the tree are shown in
faded text. These are nodes and edges that are removed from the APT as a result of
where the anchor has been moved. The elementary tree for dry/JJ shown in Figure 2
reﬂects the fact that at least some of the nouns that dry/JJ modiﬁes can be the direct
object of a verb, or the subject of a verb. When we move the anchor, as shown at the
top of Figure 3, we resolve this ambiguity to the case where the noun being modiﬁed
is a direct object. The incompatible parts of the APT are removed. This corresponds to
restricting the co-occurrence types of composed APTs to those that belong to the set
R

∗
R∗, just as was the case for elementary APTs. For example, note that in the upper
APT of Figure 3, neither the path DOBJ⋅NSUBJ from the node labeled with bought/VBD
and folded/VBD to the node labeled caused/VBD, or the path DOBJ⋅SUBJ⋅DOBJ from the node
R∗.

labeled with bought/VBD and folded/VBD to the node labeled laughter/NN are in R

Given a sufﬁciently rich elementary APT for dry/JJ, those verbs that have nouns that
dry/JJ can plausibly modify as direct objects have elementary APTs that are in some sense

∗

10

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

anchor

NSUBJ

ADVMOD

AMOD

NSUBJ

DOBJ

(a) we bought

the slightly ﬁzzy wine

DOBJ

POSS

DET

⋮
⋮
⋮
⋮
⋮

your

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮
⋮

⋮
⋮
⋮

dry
white
dry
joke
clean clothes
dry

⋮
⋮
⋮
⋮
⋮

(b)
(c) he

folded

the

caused laughter

⋮
⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

⋮
⋮

anchor

TMOD

DOBJ

POSS

DET

DET

NSUBJ

PRP

ADVMOD

AMOD

NSUBJ

XCOMP

(c)

he

folded

⋮
⋮
⋮

⋮
⋮

the

⋮
⋮

⋮
⋮
⋮
⋮
⋮ your
⋮
⋮
⋮

⋮
⋮
⋮
⋮

(d)
(e) the man hung up
(f)

boy bought

a

clothes look great
the
clothes
some very expensive clothes

wet

clothes

⋮

clean
dry

⋮

⋮
⋮
⋮
⋮

⋮
⋮
⋮
⋮

yesterday

Figure 3
The elementary APTs for dry/JJ and clothes/NNS with anchors offset.

“compatible” with the APT produced by shifting the anchor node as illustrated at the
top of Figure 3. An example is the APT for folded/VBD shown at the bottom of ﬁgure 2.
Loosely speaking, this means that when applied to the same co-occurrence type, the
APT in Figure 3 and the APT at the bottom of Figure 2 are generally expected to give
sets of lexemes with related elements.

By moving the anchors of the APT for dry/JJ and clothes/NNS as in Figure 3, we have,
in effect, aligned all of the nodes of the APTs for dry/JJ and clothes/NN with the nodes
they correspond to in the APT for folded/VBD. Not only does this make it possible, in
principle at least, to establish whether or not the composition of dry/JJ, clothes/NNS and
folded/VBD is plausible, it provides the basis for the contextualization of APTs, as we now
explain.

Recall that elementary APTs are produced by aggregating contexts taken from all
of the occurrences of the lexeme in a corpus. As described in the introduction, we
need a way to contextualize aggregated APTs in order to produce an ﬁne-grained
characterization of the distributional semantics of the lexeme in context. There are two
distinct aspects to the contextualization of APTs, both of which can be captured through
APT composition: co-occurrence ﬁltering — the down-weighting of co-occurrences that
are not compatible with the way the lexeme is being used in its current context; and

11

Computational Linguistics

Volume 1, Number 1

co-occurrence embellishment — the up-weighting of compatible co-occurrences that
appear in the APTs for the lexemes with which it is being composed.

Both co-occurrence ﬁltering and co-occurrence embellishment can be achieved
through APT composition. The process of composing the elementary APTs for the
lexemes that appear in a phrase involves two distinct steps. First, the elementary APTs
for each of the lexemes being composed are aligned in a way that is determined by the
dependency tree for the phrase. The result of this alignment of the elementary APTs,
is that each node in one of the APTs is matched up with (at most) one of the nodes in
each of the other APTs. The second step of this process involves merging nodes that
have been matched up with one another in order to produce the resulting composed
APT that represents the distributional semantics of the dependency tree. It is during
this second step that we are in a position to determine those co-occurrences that are
compatible across the nodes that have been matched up.

Figure 4 illustrates the composition of APTs on the basis of a dependency tree
shown in the upper centre of the ﬁgure. In the lower right, the ﬁgure shows the full
APT that results from merging the six aligned APTs, one for each of the lexemes in
the dependency tree. Each node in the dependency tree is labeled with a lexeme, and
around the dependency tree, we show the elementary APTs for each lexeme. The six
elementary APTs are aligned on the basis of the position of their lexeme in the depen-
dency tree. Note that the tree shown in grey within the APT is structurally identical to
the dependency tree in the upper centre of the ﬁgure. The nodes of the dependency
tree are labeled with single lexemes, whereas each node of the APT is labeled by a
weighted lexeme multiset. The lexeme labelling a node in the dependency tree is one of
the lexemes found in the weighted lexeme multiset associated with the corresponding
node within the APT. We refer to the nodes in the composed APT that come from nodes
in the dependency tree (the grey nodes) as the internal context, and the remaining nodes
as the external context.

As we have seen, the alignment of APTs can be achieved by adjusting the location
of the anchor. The speciﬁc adjustments to the anchor locations are determined by the
dependency tree for the phrase. For example, Figure 5 shows a dependency analysis of
the phrase folded dry clothes. To align the elementary APTs for the lexemes in this tree,
we do the following.

r
r
r

The anchor of the elementary APT for dry/JJ is moved to the node on which
the bought/VBD and folded/VBD lie. This is the APT shown at the top of
Figure 6. This change of anchor location is determined by the path from

the dry/JJ to folded/VBD in the tree in Figure 5, i.e. AMOD⋅DOBJ.

The anchor of the elementary APT for clothes/NNS is moved to the node on
which folded/VBD, hung/VBD and bought/VBD lie. This is the APT shown at
the bottom of Figure 3. This change of anchor location is determined by
the path from the clothes/NNS to folded/VBD in the tree in Figure 5, i.e. DOBJ.
The anchor of the elementary APT for folded/VBD has been left unchanged
because there is an empty path from from the folded/VBD to folded/VBD in the
tree in Figure 5.

Figure 6 shows the three elementary APTs for the lexemes dry/JJ, clothes/NNS and
folded/VPD which have been aligned as determined by the dependency tree shown in
Figure 5. Each column of lexemes appear at nodes that have been aligned with one
another. For example, in the third column from the left, we see that the following

12

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

Figure 4
Composition of APTs.

DOBJ

AMOD

folded/VBD dry/JJ clothes/NNS

Figure 5
A dependency tree that generates the alignment shown in Figure 6.

three nodes have been aligned: (i) the node in the elementary APT for dry/JJ at which
bought/VBD and folded/VBD appear; (ii) the node in the elementary APT for clothes/NNS
at which folded/VBD, hung/VBD and bought/VBD appear; and (iii) the anchor node of the
elementary APT for folded/VBD, i.e the node at which folded/VBD appears. In the second
phase of composition, these three nodes are merged together to produce a single node
in the composed APT.

Before we discuss how the nodes in aligned APTs are merged, we formalize the
notion of APT alignment. We do this by ﬁrst deﬁning so-called offset APTs, which
formalizes the idea of adjusting the location of an anchor. We then deﬁne how to align
all of the APTs for the lexemes in a phrase based on a dependency tree.

13

dependency treecomposed APTaligned elementary  APTselementary APTselementary APTsComputational Linguistics

Volume 1, Number 1

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

E
S
A
C

T
E
D

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

P
M
O
C
JX
B
O
D

⋮

⋮

⋮

⋮

r
e
t
h
g
u
a
l

d
e
s
⋮
u
a
c

e
k
o
j

s
e
h
t
o
l
c

e
n
w

⋮

⋮

⋮

⋮

y
a
d
r
e
t
s
e
y

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

t
a
e
⋮
r
g

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

k
o
o
l

⋮

⋮

⋮

⋮

⋮

⋮

⋮

y
r
d
n
u
a
l

⋮

⋮

⋮

e
h
t

⋮

⋮

⋮

f
o

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

s
e
h
t
o
l
c

s
e
h
t
o
l
c

s
e
h
t
o
l
c

s
e
h
t
o
l
c

s
e
h
t
o
l
c

l
l
a

⋮

⋮
i

⋮

⋮

⋮

⋮

y
z
z
ﬁ

y
r
d

e
t
i
h
w

y
r
d

n
a
e
l
c

y
r
d

n
a
⋮
e
l
c

y
r
d

t
e
w

e
v
i
s
n
e
p
x
e

n
a
⋮
e
l
c

y
r
⋮
d

⋮

⋮

⋮

⋮

⋮

y
l
t
h
g
i
l
s

⋮

⋮

⋮

⋮

y
r
e
v

⋮

⋮

⋮

⋮

⋮

⋮

⋮

e
h
⋮
t

e
h
t

⋮

e
h
⋮
t

e
h
t

e

m
o
s

⋮

⋮

e
h
⋮
t

⋮

⋮

⋮

⋮

⋮

r
u
o
y

⋮

⋮

⋮

⋮

⋮

⋮

E
S
A
C

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

r
u
o
y

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

e
r
u
s
s
e
r
p

⋮

⋮

⋮

r
e
d
n
u

⋮

⋮

⋮

⋮

p
u

⋮

⋮

⋮

p
u

⋮

⋮

⋮

⋮

⋮

t
h
g
u
o
b

e
w
⋮

d
e
d
l
o
f

e
h

⋮

d
e
d
⋮
l
o
f

g
n
u
h

⋮

e
⋮
h

n
a
m

⋮

⋮

⋮

e
h
t

t
h
g
u
o
b

y
o
b

a

d
e
d
l
o
f

e
h

d
e
d
l
o
f

e
h
s

d
e
d
l
o
f

e
h

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

⋮

)
a
(

)
b
(

)
c
(

)
c
(

)
d
(

)
e
(

)
f
(

)
c
(

)
g
(

)
h
(

⋮

⋮

⋮

⋮

D
O
M
N

T
E
D

S
S
O
P

D
O
M
N

D
O
M
T

J
B
O
D

14

J
B
U
S
N

D
O
M
A

D
O
M
V
D
A

P
R
P

J
B
U
S
N

T
E
D

e
h
t
o
t

r
e
f
e
r

t
f
e
l

e
h
t
n
o
s
t
e
k
c
a
r
b
n

i

s
r
e
t
t
e
l

.

d
e
t
t
i

m
o
n
e
e
b
e
v
a
h
s
g
a
t
h
c
e
e
p
s

e
h
T

.

5
e
r
u
g
i
F
n
i

e
e
r
t

f
o
t
r
a
p

,
s
n
o
s
a
e
r

e
c
a
p
s

e
h
t
y
b
d
e
n
i
m
r
e
t
e
d
s
a
D
B
V
/
d
e
d
l
o
f

d
n
a
S
N
N
/
s
e
h
t
o
l
c

,
J
J

/
y
r
d
r
o
f

r
o
F

.

d
e
t
c
u
r
t
s
n
o
c

e
r
a

s
T
P
A
e
s
e
h
t
h
c
i
h
w
m
o
r
f

1
e
r
u
g
i
F
n

i

n
w
o
h
s

s
e
e
r
t
y
c
n
e
d
n
e
p
e
d

s
T
P
A
d
e
n
g
i
l
a
y
l
l
a
c
i
t
r
e
V

6
e
r
u
g
i
F

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

∗

∗

∗

4.2 Offset APTs

∗

lexeme multiset).

Given some offset, δ, a string in R
setting an APT by δ involves moving the anchor to the position reached by following the

As shown in the Equation 7 below, path offset can be speciﬁed by making use of the
co-occurrence type reduction operator that was introduced in Section 2.2. Given a string
δ in R

R∗ and an APT A, the offset APT Aδ is deﬁned as follows. For each τ∈ R

R∗, the APT A when offset by δ is denoted Aδ. Off-
path δ from the original anchor position. In order to deﬁne Aδ, we must deﬁne Aδ(τ, w′)
R∗ and w′∈ V , or in terms of our alternative tree-based representation,
for each τ∈ R
we need to specify the τ′ such that Aδ(τ) and A(τ′) yield the same node (weighted
∗
R∗ and
w∈ V :
or equivalently, for each τ∈ R

Aδ(τ, w)= A(↓(δτ), w)
R∗:
Aδ(τ)= A(↓(δτ))
produces when applied to the co-occurrence type↓(δτ).
Consider the APT produced when we apply the offset AMOD⋅DOBJ. This is shown at the
top of Figure 3. Let us refer to this APT as A′. The anchor of A′ is the node at which the
lexemes bought/VDB and folded/VBD appear. Now we show how the two nodes A′(NSUBJ)
and A′(DOBJ⋅AMOD⋅ADVMOD) are deﬁned in terms of A on the basis of Equation 8. In
both cases the offset δ= AMOD⋅DOBJ.
r

As required, Equation 7 deﬁnes Aδ by specifying the weighted lexeme multiset we
get when Aδ is applied to co-occurrence type τ as being the lexeme multiset that A

As an illustrative example, consider the APT shown at the top of Figure 2. Let us
call this APT A. Note that A is anchored at the node where the lexeme dry/JJ appears.

For the case where τ= NSUBJ we have

(7)

(8)

′(NSUBJ)= A(↓(AMOD⋅DOBJ⋅NSUBJ))
= A(AMOD⋅DOBJ⋅NSUBJ)

A

r

With respect to the anchor of A, this correctly addresses the node at which
the lexemes we/PRP and he/PRP appear.

Where τ= DOBJ⋅AMOD⋅ADVMOD we have

′(DOBJ⋅AMOD⋅ADVMOD)= A(↓(AMOD⋅DOBJ⋅DOBJ⋅AMOD⋅ADVMOD))

A

= A(↓(AMOD⋅AMOD⋅ADVMOD))
= A(↓(ADVMOD))
= A(ADVMOD)

With respect to the anchor of A, this correctly addresses the node at which
the lexeme slightly/RB appears.

15

Computational Linguistics

Volume 1, Number 1

4.3 Syntax-driven APT Alignment

δi, the offset of wi in t with respect to the root, is the path in t from wi to

wh is the lexeme at the root of t. In other words, h is the position (index) in
the phrase at which the head appears;

We now make use of offset APTs, as deﬁned in Equation 7, as a way to align all of the
APTs associated with a dependency tree. Consider the following scenario:

r
w1 . . . wn is a the phrase (or sentence) where each wi∈ V for 1≤ i≤ n;
r
t∈ TV,R is a dependency analysis of the string w1 . . . wn;
r
r
wi is the elementary APT for wi for each i, 1≤ i≤ n; and
r
wh. In other words,wi, δi, wh is a co-occurrence in t for each i, 1≤ i≤ n.
Note that δh= .
We deﬁne the distributional semantics for the tree t, denotedt, as follows:
The deﬁnition of ࣶ is considered in Section 4.4. In general, ࣶ operates on a set of
w1δ1, . . . ,wnδn. It is this multiset merging operation that we focus on in Sec-
Althought can be taken to be the distributional semantics of the tree as a whole,
For each i, for 1≤ i≤ n, the APT for wi when contextualized by its role in the
dependency tree t, denotedwi; t, is the APT that satisﬁes the equality:

the same APT, when associated with different anchors (i.e. when offset in some appro-
priate way) provides a representation of each of the contextualized lexemes that appear
in the tree.

n aligned APTs, merging them into a single APT. The multiset at each node in the
resulting APT is formed by merging n multisets, one from each of the elements of

t=ࣶw1δ1, . . . ,wnδn

tion 4.4.

(9)

(10)

wi; tδi=t
wi; t=tδi

−1

Alternatively, this can also be expressed with the equality:

Note thatwh; t andt are identical. In other words, we take the representation of the

distributional semantics of a dependency tree to be the APT for the lexeme at the root
of that tree that has been contextualized by the other lexemes appearing below it in the
tree.

(11)

Equation 9 deﬁned APT composition as a “one-step” process in the sense that
all of the n elementary APTs that are associated with nodes in the dependency tree
are composed at once to produce the resulting (composed) APT. There are, however,
alternative strategies that could be formulated. One possibility is fully incremental left-
to-right composition, where, working left-to-right through the string of lexemes, the
elementary APTs for the ﬁrst two lexemes are composed, with the resulting APT then
being composed with the elementary APT for the third lexeme, and so on. It is always

16

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

possible to compose APTs in this fully incremental way, whatever the structure in the
dependency tree. The tree structure, is however, critical in determining how the adjacent
APTs need to be aligned.

4.4 Merging Aligned APTs

We now turn to the question of how to implement the functionࣶ which appears in
Equation 9.ࣶ takes a set of n aligned APTs,{ A1, . . . An}, one for each node in the
ࣶ{ A1, . . . An}, that represents the semantics of the dependency tree. Our discussion,

dependency tree t. It merges the APTs together node by node to produce a single APT,

therefore, addresses the question of how to merge the multisets that appear at nodes
that are aligned with each other and form the nodes of the APT being produced.

The elementary APT for a lexeme expresses those co-occurrences that are dis-
tributionally compatible with the lexeme given the corpus. When lexemes in some
phrase are composed, our objective is to capture the extent to which the co-occurrences
arising in the elementary APTs are mutually compatible with the phrase as a whole.
Once the elementary APTs that are being composed have been aligned, we are in a
position to determine the extent to which co-occurrences are mutually compatible: co-
occurrences that need to be compatible with one another are brought together through
the alignment. We consider two alternative ways in which this can be achieved.

bility of co-occurrences. In particular, a co-occurrence is only deemed to be compatible
with the composed lexemes to the extent that is distributionally compatible with the
lexeme that it is least compatible with. This corresponds to the multiset version of

We begin withࣶINT which provides a tight implementation of the mutual compati-
intersection. In particular, for all τ∈(R∪ R)∗ and w′∈ V :
′)= min
1≤i≤n
It is clear that the effectiveness ofࣶINT increases as the size of C grows, and that it would
An alternative to ࣶINT is ࣶUNI where we determine distributional compatibility
occurrence for each of the lexemes being composed. In particular, for all τ∈(R∪ R)∗
and w′∈ V :

particularly beneﬁt from distributional smoothing (Dagan, Pereira, and Lee 1994) which
can be used to improve plausible co-occurrence coverage by inferring co-occurrences in
the APT for a lexeme w based on the co-occurrences in the APTs of distributionally
similar lexemes.

of a co-occurrence by aggregating across the distributional compatibility of the co-

{ A1, . . . , An}(τ, w

ࣶ

Ai(τ, w

′)

(12)

INT

ࣶ

UNI

{ A1, . . . , An}(τ, w

′)= Q
1≤i≤n

Ai(τ, w

′)

(13)

While this clearly achieves co-occurrence embellishment, whether co-occurrence ﬁlter-
ing is achieved depends on the weighting scheme being used. For example, if negative
weights are allowed, then co-occurrence ﬁlter can be achieved.

There is one very important feature of APT composition that is a distinctive aspect of
our proposal, and therefore worth dwelling on. In Section 4.1, when discussing Figure 4,
we made reference to the notions of internal and external context. The internal context of
a composed APT is that part of the APT that corresponds to the nodes in the dependency
tree that generated the composed APT. One might have expected that the only lexeme

17

Computational Linguistics

Volume 1, Number 1

appearing at an internal node is the lexeme that appears at the corresponding node
in the dependency tree. However, this is absolutely not the objective: at each node in
the internal context, we expect to ﬁnd a set of alternative lexemes that are, to varying
degrees, distributionally compatible with that position in the APT. We expect that a
lexeme that is distributionally compatible with a substantial number of the lexemes
being composed will result in a distributional feature with non-zero weight in the
vectorized APT. There is, therefore, no distinction being made between internal and
external nodes. This enriches the distributional representation of the contextualized
lexemes, and overcomes the potential problem arising from the fact that as larger and
larger units are composed, there is less and less external context around to characterize
distributional meaning.

5. Experiments

In this section we consider some empirical evidence in support of APTs. First, we
consider some of the different ways in which APTs can be instantiated. Second, we
present a number of case studies showing the disambiguating effect of APT composition
in adjective-noun composition. Finally, we evaluate the model using the phrase-based
compositionality benchmarks of Mitchell and Lapata (2008) and Mitchell and Lapata
(2010).

5.1 Instantiating APTs

We have constructed APT lexicons from three different corpora.

r
r
r

clean_wiki is a corpus used for the case studies in 5.2. This corpus is a
cleaned 2013 Wikipedia dump (Wilson 2015) which we have tokenised,
part-of-speech-tagged, lemmatised and dependency-parsed using the Malt
Parser (Nivre 2004). This corpus contains approximately 0.6 billion tokens.
BNC is the British National Corpus. It has been tokenised, POS-tagged,
lemmatised and dependency-parsed as described in Grefenstette et al.
(2013) and contains approximately 0.1 billion tokens.
concat is a concatenation of the ukWaC corpus (Ferraresi et al. 2008), a
mid-2009 dump of the English Wikipedia and the British National Corpus.
This corpus has been tokenised, POS-tagged, lemmatised and
dependency-parsed as described in Grefenstette et al. (2013) and contains
about 2.8 billion tokens.

Having constructed lexicons, there are a number of hyperparameters to be explored
during composition. First there is the composition operation itself. We have explored
variants which take a union of the features such as add and max and variants which
take an intersection of the features such as mult, min and intersective_add, where

intersective_add(a, b)= a+ b iff a> 0 and b> 0; 0 otherwise.

Second, the APT theory is agnostic to the type or derivation of the weights which
are being composed. The weights in the elementary APTs can be counts, probabilities,
or some variant of PPMI or other association function. Whilst it is generally accepted
that the use of some association function such as PPMI is normally beneﬁcial in the
determination of lexical similarity, there is a choice over whether these weights should
be seen as part of the representation of the lexeme, or as part of the similarity calcu-

18

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

lation. In the instantiation which we refer to as as compose_first, APT weights are
probabilities. These are composed and transformed to PPMI scores before computing
cosine similarities. In the instantiation which we refer to as compose_second, APT
weights are PPMI scores.

There are a number of modiﬁcations that can be made to the standard PPMI calcu-
lation. First, it is common (Levy, Goldberg, and Dagan 2015) to delete rare words when
building co-occurrence vectors. Low frequency features contribute little to similarity
calculations because they co-occur with very few of the targets. Their inclusion will
tend to reduce similarity scores across the board, but have little effect on ranking.
Filtering, on the other hand, improves efﬁciency. In other experiments, we have found
that a feature frequency threshold (fft) of 1000 works well. On a corpus the size of
Wikipedia ( 1.5 billion tokens), this leads to a feature space for nouns of approximately
80, 000 dimensions (when including only ﬁrst-order paths) and approximately 230, 000
dimensions (when including paths up to order 2).

smoothing (cds), α= 0.75, can lead to performance comparable with state-of-the-art

Levy, Goldberg, and Dagan (2015) also showed that the use of context distribution

word embeddings on word similarity tasks.

PMIα(w

′

, w; τ)= log

#w, τ, w′ #∗, τ,∗α
#w, τ,∗ #∗, τ, w′α

Levy, Goldberg, and Dagan (2015) further showed that using shifted PMI, which is
analogous to the use of negative sampling in word embeddings (Levy, Goldberg, and
Dagan 2015), can be advantageous. When shifting PMI, all values are shifted down by
log k before the threshold is applied.

SPPMI(w

′

, w; τ)= max(PMI(w

′

, w; τ)− log k, 0)

Finally, there are many possible options for the path weighting function φ(τ, w).
These include the path probability p(τ  w) as discussed in Section 3, constant path

weighting, and inverse path length or harmonic function (which is equivalent to the
dynamic context window used in many neural implementations such as GloVe (Pen-
nington, Socher, and Manning 2014)).

5.2 Disambiguation

Here we consider the differences between using aligned and unaligned APT repre-

sentations as well as the differences between usingࣶUNI andࣶINT when carrying out

adjective-noun (AN) composition. From the clean_wiki corpus described in Section
5.1, a small number of high frequency nouns were chosen which are ambiguous or
broad in meaning together with potentially disambiguating adjectives. We use the
compose_first option described above where composition is carried out on APTs
containing probabilities.

W(w, τ, w

′)= #w, τ, w′
#w,∗,∗

19

Computational Linguistics

Volume 1, Number 1

The closest distributional neighbours of the individual lexemes before and after
composition with the disambiguating adjective are then examined. In order to calcu-
late similarities, contexts are weighted using the variant of PPMI advocated by Levy,

Goldberg, and Dagan (2015) where the context distribution is smoothed with α= 0.75

(no shifting since we have found shifting to have little effect when working with rela-
tively small corpora). Similarity is then computed using the standard cosine measure.
For illustrative purposes the top ten neighbours of each word or phrase are shown,
concentrating on ranks rather than absolute similarity scores.

AlignedࣶUNI

UnalignedࣶUNI

shoot
shot
leaf
shooting
ﬁght
scene
video
tour
footage
interview
ﬂower

green shoot
shoot
leaf
ﬂower
fruit
orange
tree
color
shot
colour
cover

six-week shoot
shoot
tour
shot
break
session
show
shooting
concert
interview
leaf

green shoot
shoot
shot
leaf
shooting
ﬁght
scene
video
tour
ﬂower
footage

six-week shoot
shoot
shot
shooting
leaf
scene
video
ﬁght
footage
photo
interview

Table 2
Neighbours of uncontextualised shoot/N compared to shoot/N in the contexts of green/J and

six-week/J, usingࣶUNI with aligned and unaligned representations

AlignedࣶINT

UnalignedࣶINT

shoot
shot
leaf
shooting
ﬁght
scene
video
tour
ﬂower
footage
interview

green shoot
shoot
leaf
fruit
stalk
ﬂower
twig
sprout
bud
shrub
inﬂorescence

six-week shoot
shoot
photoshoot
taping
tour
airing
rehearsal
broadcast
session
q&a
post-production

green shoot
shoot
pyrite
plosive
handlebars
annual
roundel
affricate
phosphor
connections
reduplication

six-week shoot
e/f
uemtsu
confederations
shortlist
all-ireland
dern
gerwen
tactics
backstroke
gabler

Table 3
Neighbours of uncontextualised shoot/N compared to shoot/N in the contexts of green/J and

six-week/J, usingࣶINT with aligned and unaligned representations
Table 2 illustrates what happens whenࣶUNI is used to merge aligned and unaligned

APT representations when the noun shoot is placed in the contexts of green and six-week.
Boldface is used in the entries of compounds where a neighbour appears to be highly
suggestive of the intended sense and where it has a rank higher or equal to its rank in

20

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

representations (see Table 3).

generally leads to substantially degraded neighbours, often little better than random,
as illustrated by Table 3.

see the disambiguating effect of the adjective. A green shoot is more similar to leaf,
ﬂower, fruit and tree. A six-week shoot is more similar to tour, session, show and concert.

the entry for the uncontextualised noun. In this example, it is clear that merging the
unaligned APT representations provides very little disambiguation of the target noun.
This is because typed co-occurrences for an adjective mostly belong in a different space
to typed co-occurrences for a noun. Addition of these spaces leads to signiﬁcantly lower
absolute similarity scores, but little change in the ranking of neighbours. Whilst we
only show one example here, this observation appears to hold true whenever words

ing aligned APT representations. Again, boldface is used in the entries of compounds
where a neighbour appears to be highly suggestive of the intended sense and where it
has a rank higher or equal to its rank in the entry for the uncontextualised noun. In these

with different part of speech tags are composed. Intersection of these spaces viaࣶINT
On the other hand when APTs are correctly aligned and merged usingࣶUNI, we
This disambiguating effect is even more apparent whenࣶINT is used to merge the APT
Table 4 further illustrates the difference between usingࣶUNI andࣶINT when compos-
examples, we can see that bothࣶUNI andࣶINT appear to be effective in carrying out some
disambiguation. Looking at the example of musical group, bothࣶUNI andࣶINT increase
However, ࣶINT also leads to a number of other words being selected as neighbours
This is not the case whenࣶUNI is used — the other neighbours still appear related to
such as ethnic group, human body and magnetic ﬁeld. Further, even whenࣶUNI leads to
whenࣶINT is used.
that ࣶUNI should increase the number of non-zero dimensions whereas ࣶINT should
based onࣶUNI behave like high frequency words and composed representations based
on ࣶINT behave like low frequency words. Further, when using similarity measures

The reason for this is likely to be the effect that each of these composition operations
has on the number of non-zero dimensions in the composed representations. Ignoring
the relatively small effect the feature association weight may have on this, it is obvious

the successfully selection of a large number of sense speciﬁc neighbours, e.g. see literary
work, the neighbours selected appear to be higher frequency, more general words than

the relative similarity of band and music to group when it is contextualised by musical.

which are closely related to the musical sense of group e.g. troupe, ensemble and trio.

the general meaning of group. This trend is also seen in some of the other examples

decrease the number of non-zero dimensions. In general, the number of non-zero di-
mensions is highly correlated with frequency, which makes composed representations

based on PPMI, as demonstrated by Weeds (2003), it is not unusual to ﬁnd that the
neighbours of high frequency entities (with a large number of non-zero dimensions) are
other high frequency entities (also with a large number of non-zero dimensions). Nor is
it unusual to ﬁnd that the neighbours of low frequency entities (with a small number
of non-zero dimensions) are other low frequency entities (with a small number of non-
zero dimensions). Weeds, Weir, and McCarthy (2004) showed that frequency is also a

surprisingly good indicator of the generality of the word. HenceࣶUNI leads to more
general neighbours andࣶINT leads to more speciﬁc neighbours.
Finally, note that whilstࣶINT has produced high quality neighbours in these exam-
ples where only two words are composed, usingࣶINT in the context of the composition
of the internal nodes of the APT composed using an intersective operation such asࣶINT
must necessarily only include the lexemes actually used in the sentence.ࣶUNI on the

of an entire sentence would tend to lead to very sparse representations. The majority

21

Computational Linguistics

Volume 1, Number 1

AlignedࣶUNI

AlignedࣶINT

group
group
organization
organisation
company
community
corporation
unit
movement
association
society
body
body
board
organization
entity
skin
head
organisation
structure
council
eye
work
study
project
book
activity
effort
publication
job
program
writing
piece
ﬁeld
facility
stadium
area
complex
ground
pool
base
space
centre
park

musical group
group
band
troupe
ensemble
artist
trio
genre
music
duo
supergroup
human body
body
organism
organization
entity
embryo
brain
community
organelle
institution
cranium
social work
work
research
study
writings
endeavour
project
discourse
topic
development
teaching
athletic ﬁeld
ﬁeld
gymnasium
ﬁeldhouse
stadium
gym
arena
rink
softball
cafeteria
ballpark

ethnic group
group
community
organization
grouping
sub-group
faction
ethnicity
minority
organisation
tribe
legislative body
body
council
committee
board
legislature
secretariat
authority
assembly
power
ofﬁce
literary work
work
writings
treatise
essay
poem
book
novel
monograph
poetry
writing
magnetic ﬁeld
ﬁeld
wavefunction
spacetime
ﬂux
subﬁeld
perturbation
vector
e-magnetism
formula_8
scalar

musical group
group
company
band
music
movement
community
society
corporation
category
association
human body
body
organization
structure
entity
organisation
skin
brain
eye
object
organ
social work
work
activity
study
project
program
practice
development
aspect
book
effort
athletic ﬁeld
ﬁeld
facility
stadium
gymnasium
basketball
sport
center
softball
gym
arena

ethnic group
group
organization
organisation
community
company
movement
society
minority
unit
entity
legislative body
body
council
committee
board
authority
assembly
organisation
agency
commission
entity
literary work
work
book
study
novel
project
publication
text
literature
story
writing
magnetic ﬁeld
ﬁeld
component
stadium
facility
track
ground
system
complex
parameter
pool

Distributional Neighbors usingࣶUNI vsࣶINT (e-magnetism = electro-magnetism)

Table 4

22

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

other hand will have added to these internal representations, suggesting similar words
which might have been used in those contexts and giving rise to a rich representation
which might be used to calculate sentence similarity. Further, the use of PPMI, or some
other similar form of feature weighting and selection, will mean that those internal (and
external) contexts which are not supported by a majority of the lexemes in the sentence
will tend to be considered insigniﬁcant and therefore will be ignored in similarity
calculations. By using shifted PPMI, it should be possible to further reduce the number

of non-zero dimensions in a representation constructed usingࣶUNI which should also

allow us to control the speciﬁcity/generality of the neighbours observed.

5.3 Phrase-based Composition Tasks

Here we look at the performance of one instantiation of the APT framework on two
benchmark tasks for phrase-based composition.

5.3.1 Experiment 1: the M&L2010 dataset. The ﬁrst experiment uses the M&L2010
dataset, introduced by Mitchell and Lapata (2010), which contains human similarity
judgements for adjective-noun (AN), noun-noun (NN) and verb-object (VO) combina-
tions on a seven-point rating scale. It contains 108 combinations in each category such

associal activity, economic condition,tv set, bedroom window andﬁght war, win battle.

This dataset has been used in a number of evaluations of compositional methods in-
cluding Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann
and Blunsom (2013) and Kiela et al. (2014). For example, Blacoe and Lapata (2012) show
that multiplication in a simple distributional space (referred to here as an untyped VSM)
outperforms the distributional memory (DM) method of Baroni and Lenci (2010) and
the neural language model (NLM) method of Collobert and Weston (2008).

Whilst often not explicit, the experimental procedure in most of this work would
appear to be the calculation of Spearman’s rank correlation coefﬁcient ρ between model
scores and individual, non-aggregated, human ratings. For example, if there are 108
phrase pairs being judged by 6 humans, this would lead to a dataset containing 648
data points. The procedure is discussed at length in Turney (2012), who argues that this
method tends to underestimate model performance. Accordingly, Turney explicitly uses
a different procedure where a separate Spearman’s ρ is calculated between the model
scores and the scores of each participant. These coefﬁcients are then averaged to give
the performance indicator for each model. Here, we report results using the original
M&L method, see Table 5. We found that using the Turney method scores were typically
higher by 0.01 to 0.04. If model scores are evaluated against aggregated human scores,
then the values of Spearman’s ρ tends to be still higher, typically 0.1 to 0.12 higher than
the values reported here.

For this experiment, we have constructed an order 2 APT lexicon for the BNC corpus.
This is the same corpus used by Mitchell and Lapata (2010) and for the best performing
algorithms in Blacoe and Lapata (2012). We note that the larger concat corpus was used
by Blacoe and Lapata (2012) in the evaluation of the DM algorithm (Baroni and Lenci
2010). We use the compose_second option described above where the elementary
APT weights are PPMI. With regard to the different parameter settings in the PPMI
calculation (Levy, Goldberg, and Dagan 2015), we tuned on a number of popular word
similarity tasks: MEN (Bruni, Tran, and Baroni 2014); WordSim-353 (Finkelstein et al.
2002); and SimLex-999 (Hill, Reichart, and Korhonen 2015). In these tuning experiments,
we found that context distribution smoothing gave mixed results. However, shifting

PPMI (k= 10) gave optimal results across all of the word similarity tasks. Therefore we

23

Computational Linguistics

Volume 1, Number 1

ࣶINT, k= 1
ࣶINT, k= 10
ࣶUNI, k= 1
ࣶUNI, k= 10

untyped VSM, multiply
(Mitchell and Lapata 2010)
untyped VSM, multiply
(Blacoe and Lapata 2012)
distributional memory (DM), add
(Blacoe and Lapata 2012)
neural language model (NLM), add
(Blacoe and Lapata 2012)
humans
(Mitchell and Lapata 2010)

AN NN VO Average
0.23
-0.09
0.43
0.16
NaN 0.23
0.41
0.37
0.47
0.45
0.42
0.43
0.44
0.49
0.46

0.35
0.26
0.40
0.42
0.37

0.48

0.50

0.35

0.37

0.30

0.29

0.28

0.26

0.24

0.52

0.49

0.55

0.44

0.32

0.26

0.52

Table 5
Results on the M&L2010 dataset using the M&L method of evaluation. Values shown are
Spearman’s ρ.

report results here for vanilla PPMI (shift k= 1) and shifted PPMI (shift k= 10). For
composition, we report results for bothࣶUNI andࣶINT. Results are shown in Table 5.
For this task and with this corpus ࣶUNI consistently outperforms ࣶINT. Shifting
PPMI by log 10 consistently improves results forࣶUNI, but has a large negative effect
on the results forࣶINT. We believe that this is due to the relatively small size of the
Comparing these results with the state-of-the-art, we can see that ࣶUNI clearly

corpus. Shifting PPMI reduces the number of non-zero dimensions in each vector which
increases the likelihood of a zero intersection. In the case of AN composition, all of the
intersections were zero for this setting, making it impossible to compute a correlation.

outperforms DM and NLM as tested by Blacoe and Lapata (2012). This method of
composition is also achieving close to the best results in Mitchell and Lapata (2010)
and Blacoe and Lapata (2012). It is interesting to note that our model does substantially
better than the state-of-the-art on verb-object composition, but is considerably worse
at noun-noun composition. Exploring why this is so is a matter for further research.
We have undertaken experiments with a larger corpus and a larger range of hyper-
parameter settings which indicate that the performance of the APT models can be
increased signiﬁcantly. However, these results are not presented here, since an equatable
comparison with existing models would require a similar exploration of the hyper-
parameter space across all models being compared.

5.3.2 Experiment 2: the M&L2008 dataset. The second experiment uses the M&L2008
dataset, introduced by Mitchell and Lapata (2008), which contains pairs of intransitive
sensitives together with human judgments of similarity. The dataset contains 120 unique
subject, verb, landmark triples with a varying number of human judgments per item.
On average each triple is rated by 30 participants. The task is to rate the similarity of the
verb and the landmark given the potentially disambiguating context of the subject. For

24

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

example, in the context of the subject ﬁre one might expect glowed to be close to burned
but not close to beamed. Conversely, in the context of the subject face one might expect
glowed to be close to beamed and not close to burned.

This dataset was used in the evaluations carried out by Grefenstette et al. (2013)
and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental
procedure of Mitchell and Lapata and do not evaluate against mean scores. Instead,
separate points are created for each human annotator, as discussed in Section 5.3.1.

on this dataset. In the evaluation of Dinu, Pham, and Baroni (2013), the lexical function
algorithm, which learns a matrix representation for each functor and deﬁnes composi-
tion as matrix-vector multiplication, was the best performing compositional algorithm

The multi-step regression algorithm of Grefenstette et al. (2013) achieved ρ= 0.23
at this task. With optimal parameter settings, it achieved around ρ= 0.26. In this evalu-
ation, the full additive model of Guevara (2010) achieved ρ< 0.05.
in Section 5.3.1. As before note that k= 1 in shifted PPMI is equivalent to not shifting
ࣶINT, k= 1
ࣶINT, k= 10
ࣶUNI, k= 1
ࣶUNI, k= 10

In order to make our results directly comparable with these previous evaluations,
we have used the same corpus to construct our APT lexicons, namely the concat cor-
pus described in Section 5.1. Otherwise, the APT lexicon was constructed as described

PPMI. Results are shown in Table 6.

0.23
0.13
0.20
0.26
0.23

0.23–0.26

0.20-0.22

0–0.05

0.40

multi-step regression
Grefenstette et al. (2013)
lexical function
Dinu, Pham, and Baroni (2013)
untyped VSM, mult
Dinu, Pham, and Baroni (2013)
full additive
Dinu, Pham, and Baroni (2013)
humans
Mitchell and Lapata (2008)

Table 6
Results on the M&L2008 dataset. Values shown are Spearman’s ρ.

which was the best performing model in the evaluation of Dinu, Pham, and Baroni
(2013). In that evaluation, the lexical function model achieved between 0.23 and 0.26
depending on the parameters used in dimensionality reduction. Using vanilla PPMI,

We see thatࣶUNI is highly competitive with the optimised lexical function model
without any context distribution smoothing or shifting,ࣶUNI achieves ρ= 0.20, which
is less thanࣶINT. However, when using shifted PPMI as weights, the best result is 0.26.
considered as features. This makes sense when using an additive model such asࣶUNI.
We also see that at this task and using this corpusࣶINT performs relatively well.
ρ= 0.23 which equals the performance of the multi-step regression algorithm Grefen-

Using vanilla PPMI, without any context distribution smoothing or shifting, it achieves

The shifting of PPMI means that contexts need to be more surprising in order to be

25

Computational Linguistics

Volume 1, Number 1

stette et al. (2013). Here, however, shifting PPMI has a negative impact on performance.
This is largely due to the intersective nature of the composition operation — if shifting
PPMI removes a feature from one of the unigram representations, it cannot be recovered
during composition.

6. Related Work

Our work brings together two strands usually treated as separate though related prob-
lems: representing phrasal meaning by creating distributional representations through
composition; and representing word meaning in context by modifying the distributional
representation of a word. In common with some other work on lexical distributional
similarity, we use a typed co-occurrence space. However, we propose the use of higher-
order grammatical dependency relations to enable the representation of phrasal mean-
ing and the representation of word meaning in context.

6.1 Representing Phrasal Meaning

The problem of representing phrasal meaning has traditionally been tackled by tak-
ing vector representations for words (Turney and Pantel 2010) and combining them
using some function to produce a data structure that represents the phrase or sen-
tence. Mitchell and Lapata (2008, 2010) found that simple additive and multiplicative
functions applied to proximity-based vector representations were no less effective than
more complex functions when performance was assessed against human similarity
judgements of simple paired phrases.

The word embeddings learnt by the continuous bag-of-words model (CBOW) and
the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently
among the most popular forms of distributional word representations. Whilst using
a neural network architecture, the intuitions behind such distributed representations
of words are the same as in traditional distributional representations. As argued by
Pennington et al. (2014), both count-based and prediction-based models probe the
underlying corpus co-occurrences statistics. For example, the CBOW architecture pre-
dicts the current word based on context (which is viewed as a bag-of-words) and the
skip-gram architecture predicts surrounding words given the current word. Mikolov
et al. (2013c) showed that it is possible to use these models to efﬁciently learn low-
dimensional representations for words which appear to capture both syntactic and
semantic regularities. Mikolov et al. (2013b) also demonstrated the possibility of com-
posing skip-gram representations using addition. For example, they found that adding
the vectors for Russian and river results in a very similar vector to the result of adding
the vectors for Volga and river. This is similar to the multiplicative model of Mitchell and
Lapata (2008) since the sum of two skip-gram word vectors is related to the product of
two word context distributions.

Whilst our model shares with these the use of vector addition as a composition
operation, the underlying framework is very different. Speciﬁcally, the actual vectors
added depend not just on the form of the words but also their grammatical relationship
within the phrase or sentence. This means that the representation for, say, glass window
is not equal to the representation of window glass. The direction of the NN relationship
between the words leads to a different alignment of the APTs and consequently a
different representation for the phrases.

There are other approaches which incorporate theoretical ideas from formal seman-
tics and machine learning, use syntactic information, and specialise the data structures

26

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

to the task in hand. For adjective-noun phrase composition, Baroni and Zamparelli
(2010) and Guevara (2010) borrowed from formal semantics the notion that an adjective
acts as a modifying function on the noun. They represented a noun as a vector, an
adjective as a matrix, which could be induced from pairs of nouns and adjective noun
phrases, and composed the two using matrix-by-vector multiplication to produce a
vector for the noun phrase. Separately, Coecke, Sadrzadeh, and Clark (2011) proposed
a broader compositional framework that incorporated from formal semantics the no-
tion of function application derived from syntactic structure (Montague 1970; Lambek
1999). These two approaches were subsequently combined and extended to incorporate
simple transitive and intransitive sentences, with functions represented by tensors, and
arguments represented by vectors (Grefenstette et al. 2013).

The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli
(2010) approach; all words, regardless of part-of-speech, were modelled with both a
vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and
Clark (2011) in using syntax to guide the order of phrasal composition. This model,
however, was made much more ﬂexible by requiring and using task-speciﬁc labelled
training data to create task-speciﬁc distributional data structures, and by allowing non-
linear relationships between component data structures and the composed result. The
payoff for this increased ﬂexibility has come with impressive performance in sentiment
analysis (Socher et al. 2012, 2013).

However, whilst these approaches all pay attention to syntax, they all require large
amounts of training data. For example, running regression models to accurately predict
the matrix or tensor for each individual adjective or verb requires a large number of
exemplar compositions containing that adjective or verb. Socher’s MV-RNN model
further requires task-speciﬁc labelled training data. Our approach, on the other hand,
is purely count-based and directly aggregates information about each word from the
corpus.

Other approaches have been proposed. Clarke (2007, 2012) suggested a context-
theoretic semantic framework, incorporating a generative model that assigned prob-
abilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh,
and Clark (2011) an ambition to provide a bridge between compositional distribu-
tional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and
Mooney (2011) combined word-level distributional vector representations with logic-
based representation using a probabilistic reasoning framework. Lewis and Steedman
(2013) also attempted to combine distributional and logical semantics by learning a
lexicon for CCG (Combinatory Categorial Grammar (Steedman 2000)) which ﬁrst maps
natural language to a deterministic logical form and then performs a distributional clus-
tering over logical predicates based on arguments. The CCG formalism was also used
by Hermann and Blunsom (2013) as a means for incorporating syntax-sensitivity into
vector space representations of sentential semantics based on recursive auto-encoders
(Socher et al. (2011a, 2011b)). They achieved this by representing each combinatory step
in a CCG parse tree with an auto-encoder function, where it is possible to parameterise
both the weight matrix and bias on the combinatory rule and the CCG category.

Turney (2012) offered a model that incorporated assessments of word-level semantic
relations in order to determine phrasal-level similarity. This work uses two different
word-level distributional representations to encapsulate two types of similarity, and
captures instances where the components of a composed noun phrase bore similarity
to another word through a mix of those similarity types. Crucially, it views similarity
of phrases as a function of the similarities of the components and does not attempt to
derive modiﬁed vectors for phrases or words in context. Dinu and Thater (2012) also

27

Computational Linguistics

Volume 1, Number 1

compared computing sentence similarity via additive compositional models with an
alignment-based approach, where sentence similarity is a function of the similarities of
component words, and simple word overlap. Their results showed that a model based
on a mixture of these approaches outperformed all of the individual approaches on a
number of textual entailment datasets.

6.2 Typed Co-occurrence Models

In untyped co-occurrence models, such as those considered by Mitchell and Lapata
(2008, 2010) , co-occurrences are simple, untyped pairs of words which co-occur together
(usually within some window of proximity but possibly within some grammatical
relation). The lack of typing makes it possible to compose vectors through addition and
multiplication. However, in the computation of lexical distributional similarity using
grammatical dependency relations, it has been typical (Lin 1998b; Lee 1999; Weeds and
Weir 2005) to consider the type of a co-occurrence (for example, does dog occur with eat
as its direct object or its subject?) as part of the feature space. The distinction between
vector spaces based on untyped and typed co-occurrences was formalised by Padó
and Lapata (2007) and Baroni and Lenci (2010). In particular, Baroni and Zamparelli
(2010) showed that typed co-occurrences based on grammatical relations were better
than untyped co-occurrences for distinguishing certain semantic relations. However,
as shown by Weeds, Weir, and Refﬁn (2014), it does not make sense to compose typed
features based on ﬁrst-order dependency relations through multiplication and addition,
since the vector spaces for different parts of speech are largely non-overlapping.

Padó and Lapata (2007) constructed features using higher-order grammatical de-
pendency relations. They deﬁned a path through a dependency tree in terms of the
node words. This allowed words which are only indirectly related within a sentence to
be considered as co-occurring. For example, in a lorry carries apples, there is a path of
length 2 between the nouns lorry and apples via the node carry. However, they also used
a word-based basis mapping which essentially reduces all of the salient grammatical

paths to untyped co-occurrences. Given the pathslorry, carry andlorry, carry, apples

for lorry, these would be mapped to the basis elements carry and apples respectively.

6.3 Representing Word Meaning in Context

A long-standing topic in distributional semantics has been the modiﬁcation of a canon-
ical representation of a lexeme’s meaning to reﬂect the context in which it is found.
Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and
the vector then modiﬁed to reﬂect the instance context (Lund and Burgess 1996; Erk and
Padó 2008; Mitchell and Lapata 2008; Thater, Dinu, and Pinkal 2009; Thater, Fürstenau,
and Pinkal 2010, 2011; Van de Cruys, Poibeau, and Korhonen 2011; Erk 2012).

As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically
been modiﬁed using simple additive and multiplicative compositional functions. Other
approaches, however, share with our proposal the use of syntax to drive modiﬁcation
of the distributional representation (Erk and Padó 2008; Thater, Dinu, and Pinkal 2009;
Thater, Fürstenau, and Pinkal 2010, 2011).

Erk and Padó (2008) introduced a structured vector space model of word meaning
that computes the meaning of a word in the context of another word via selectional
preferences. This approach was shown to work well at ranking paraphrases taken from
the SemEval-2007 lexical substitution task (McCarthy and Navigli 2007). In the Erk &
Padó approach, the meaning of ball in the context of the phrase catch ball is computed by

28

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

combining the lexical vector for ball with the object preference vector of catch i.e. things
which can be caught. Whilst this approach is based on very similar intuitions to ours, it
is in fact quite different. The lexical vector which is modiﬁed is not the co-occurrence
vector, as in our model, but a vector of neighbours computed from co-occurrences. For
example, the lexical vector for catch in the Erk & Padó approach might contain throw,
catch and organise. These neighbours of catch are then combined with verbs which have
been seen with ball in the direct object relation using vector addition or component-
wise multiplication. Thus, it is possible to carry out this approach with reference only to
observed ﬁrst order grammatical dependency relationship. In their experiments, they
used the “dependency-based” vector space of Padó and Lapata (2007) where target and
context words are linked by a valid dependency path (i.e. not necessarily a single ﬁrst-
order grammatical relation). However, higher-order dependency paths were purely
used to provide extra contexts for target words, than would be seen in a traditional
ﬁrst-order dependency model, during the computation of neighbour sets. Further, the
Erk & Padó approach does not construct a representation of the phrase since this model
is focussed on lexical disambiguation rather than composition and it is not obvious how
one would carry out further disambiguations within the context of a whole sentence.

More recently, Thater, Fürstenau, and Pinkal (2011) used a similar approach but
considered a broader range of operations for combining two vectors where individual
vector components are reweighted. Speciﬁcally, they found that reweighting vector
components based on the distributional similarity score between words deﬁning vector
components and the observed context words led to improved performance at ranking
paraphrases.

Thater, Fürstenau, and Pinkal (2010) noted that vectors of two syntactically related
words typically have different syntactic environments, making it difﬁcult to combine
information in the respective vectors. They build on Thater, Dinu, and Pinkal (2009),
where the meaning of argument nouns was modelled in terms of the predicates they
co-occur with (referred to as a ﬁrst-order vector) and the meaning of predicates in terms
of second-order co-occurrence frequencies with other predicates. These predicate vectors
can be obtained by adding argument vectors. For example, the verb catch will contain
counts on the dimension for kick introduced by the direct-object ball and counts on the
dimension for contract introduced by the direct-object cold. In other words, like in the
Erk & Pado approach, the vector for a verb can be seen as a vector of similar verbs,
thus making this notion of second-order dependency compatible with that used in work
on word sense discrimination (Schütze 1998) rather than referring to second-order (or
higher order) grammatical dependencies as in this work. Contextualisation can then
be achieved by multiplication of a second-order predicate vector with a ﬁrst-order
argument vector since this selects the dimensions which are common to both. Thater,
Fürstenau, and Pinkal (2010) presented a more general model where every word is
modelled in terms of ﬁrst-order and second-order co-occurrences and demonstrate high
performance at ranking paraphrases.

7. Directions for Future Work

7.1 Representations

There are a number of apparent limitations of our approach that are simply a reﬂection
of our decision to adopt dependency-based syntactic analysis.

First, surface disparities in syntactic structure (e.g. active versus passive tense
formations, compound sentence structures) will disrupt sentence-level comparisons

29

Computational Linguistics

Volume 1, Number 1

using a simple APT structure based on surface dependency relations, but this can be
addressed, for example, by syntax-based pre-processing. The APT approach is agnostic
in this regard.

Second, traditional dependency parsing does not distinguish between the order of
modiﬁers. Hence the phrases happiest blonde person and blonde happiest person receive the
same dependency representation and therefore also the same semantic representation.
However, we believe that our approach is ﬂexible enough to be able to accommodate a
more sensitive grammar formalism which does allow for distinctions in modiﬁer scope
to be made if an application demands it. In future work we intend to look at other
grammar formalisms including CCG (Steedman 2000).

By proposing a count-based method for composition we are bucking the growing
trend of working with prediction-based word embeddings. Whilst there has been initial
evidence (Baroni, Dinu, and Kruszewski 2014) that prediction-based methods are supe-
rior to count-based methods at the lexeme level e.g. for synonym detection and concept
categorisation, there is more recent evidence (Levy and Goldberg 2014) that the skip-
gram model with negative sampling as introduced in Mikolov et al. (2013a) is equivalent
to implicit factorisation of the PPMI matrix. Levy, Goldberg, and Dagan (2015) also
demonstrated how traditional count-based methods could be improved by transferring
hyperparameters used by the prediction-based methods (such as context distribution
smoothing and negative sampling). This led to the count-based methods outperforming
the prediction-based methods on a number of word similarity tasks. A next step for us
is to take the lessons learnt from work on word embeddings and ﬁnd a way to produce
lower dimensionality APT representations without destroying the necessary structure
which drives composition. The advantages of this from a computational point of view
are obvious. It remains to be seen what effect the improved generalization also promised
by dimensionality reduction will have on composition via APTs.

By considering examples, we have seen that composition of APTs using both union
and intersection can lead to nearest neighbours which are clearly disambiguating. On
benchmark phrase-based composition tasks, the performance of union in APT composi-
tion is close to or equalling the state-of-the-art on those tasks. However, we believe that
the performance of intersection in APT composition is currently limited by the impov-
erished nature of word representations based directly on corpus statistics. Even given a
very large corpus, there are always many plausible co-occurrences which have not been
observed. One possible solution, which we explore elsewhere, is to smooth the word
representations using their distributional neighbours before applying an intersective
composition operation.

7.2 Applications

In Section 5.2, we demonstrated the potential for using APTs to carry out word sense
disambiguation / induction. Uncontextualised, elementary APTs typically contain a
corpus-determined mixture of co-occurrences referencing different usages. The APT
generated by a dependency tree, however, provides contextualised lexeme representa-
tions where the weights have been adjusted by the inﬂuence of the contextual lexemes
so that the co-occurrences relating to the correct usage have been appropriately up-
weighted, and the co-occurrences found in other circumstances down-weighted. In
other words, APT structures automatically perform word sense induction on lexeme-
level representations which is demonstrable through the lexeme similarity measure. For
example, we observed that the contextualised lexeme representation of body in the APT
constructed by embedding it in the phrase human body had a relatively high similarity

30

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

to the uncontextualised representation of brain and a relatively low similarity to council,
while the equivalent lexeme representation for body embedded in the APT constructed
for the phrase legislative body showed the reverse pattern.

One common criticism of distributional thesauruses is that they conﬂate different
semantic relations into a single notion of similarity. For example, when comparing
representations based on grammatical dependency relations, the most similar word to
an adjective such as hot will usually be found to be its antonym cold. This is because
hot and cold are both used to modify many of the same nouns. However, if as in the
APT framework, the representation of cold includes not only the direct dependents
of cold, but also the indirect dependents, e.g. verbs which co-occur with cold things,
it is possible that more differences between its representation and that of hot might
be found. One would imagine that the things which are done to hot things are more
different to the things which are done to cold things than they are to the things which
are done to very warm things. Further, the examples in Section 5.2 raises the possibility
that different composition operations might be used to distinguish different semantic

relations including hypernyms, hyponyms and co-hyponyms. For example,ࣶUNI tends
to lead to more general neighbours (e.g. hypernyms) andࣶINT tends to lead to more

speciﬁc neighbours (e.g. hyponyms).

Phrase-level or sentence-level plausibility measures offer the prospect of a continu-
ous measure of the appropriateness / plausibility of a complete phrase or sentence, based
on a combination of semantic and syntactic dependency relations. APTs offer a way to
measure the plausibility of a lexeme when embedded in a dependency tree, suggesting
that APTs may be successfully employed in tackling sentence completion tasks, such as
the Microsoft Research Sentence Completion Challenge (Zweig and Burges 2012). Here
the objective is to identify the word that will ﬁll out a partially completed sentence in
the best possible way. For example, is ﬂurried or proﬁtable the best completion of the
sentence below.

"Presently he emerged looking even more [ﬂurried / proﬁtable] than before."

We can compose the APTs for the partially completed sentence. Comparing the result
with the elementary APTs for each of the candidates should provide a good, direct
measurement of which candidate is more plausible. An improved language model has
implications for parsing, speech recognition and machine translation.

A central goal of compositional distributional semantics is to create a data structure
that represents an entire phrase or sentence. The composed APT for a dependency tree
provides such a structure, but leaves open the question as to how this structure might
be exploited for phrase-level or sentence-level semantic comparison.

The ﬁrst point to be made is that, unusually, we have available not only a represen-
tation of the whole dependency tree but also contextualised (vector) representations for
the lexemes in the dependency tree. This makes available to us any analytical technique
which requires separate analysis of lexical components of the phrase or sentence. How-
ever, this leads to the problem of how to read the structure at the global phrase/sentence-
level.

For similarity measures, one straightforward option would be to create a vector
from the APT anchored at the head of the phrase or sentence being considered. Thus
the phrasal vector for a red rose would be created taking the node containing rose as the
anchor. In other words, the vector representation of the phrase a red rose will be the same
as the contextualised representation of rose. Similarly, the vector representation for the

31

Computational Linguistics

Volume 1, Number 1

sentence he took the dog for a walk will be the same as the contextualised representation
of the verb took.

Such a representation provides a continuous model of similarity (and meaning)
at the phrasal-level and/or sentence-level. We anticipate that vector comparisons of
phrase or sentence-level vectors produced in this manner will provide some coherent
numerical measure of distributional similarity. This approach should be useful for
paraphrase recognition tasks. For example, in order to identify good candidate para-
phrases for questions in a question-answering task, Berant and Liang (2014) employ a
paraphrase model based on adding word embeddings constructed using the CBOW
model of Mikolov et al. (2013). Whilst the authors achieve state-of-the-art using a
mixture of methods, a paraphrase model based on the addition of vectors based on
untyped co-occurrences alone cannot distinguish meanings where syntax is important.
For example, the sentences Oswald shot Kennedy and Kennedy shot Oswald would have
the same representations. On the other hand, APT composition is syntax-driven and
will provide a representation of each sentence which is sensitive to lexical meaning and
syntax.

Another advantage of using APT composition in paraphrase recognition, over some
other syntax-driven proposals, is that the same structure is used to represent words,
phrases and sentences. Provided the head node is of the same type of speech, words
and phrases of different lengths can easily be compared within our model. An adjective-
noun compound such as male sibling is directly comparable with the single noun brother.
Further, there is no need for there to be high similarity between aligned components of
phrases or sentences. For example, the phrase female scholar can be expected to have
a high similarity with the phrase educated woman, in terms at least of their external
contexts.

8. Conclusions

This paper presents a new theory of compositional distributional semantics. It employs
a single structure, the APT, which can represent the distributional semantics of lexemes,
phrases and even sentences. By retaining higher-order grammatical structure in the
representations of lexemes, composition captures mutual disambiguation and mutual
generalisation of constituents. APTs allow lexemes and phrases to be compared in
isolation or in context. Further, we have demonstrated how one instantiation of this
theory can achieve results which are very competitive with state-of-the-art results on
benchmark phrase-based composition tasks.

As we have discussed, APTs have a wide range of potential applications including
word sense induction, word sense disambiguation, parse reranking, dependency pars-
ing and language modelling more generally, and also paraphrase recognition. Further
work is required to gain an understanding of which instantiations of the theory are
suited to each of these applications.

9. Acknowledgements

This work was funded by UK EPSRC project EP/IO37458/1 “A Uniﬁed Model of Compo-
sitional and Distributional Compositional Semantics: Theory and Applications”. We would
like to thank all members of the DISCO project team. Particular thanks to Miroslav
Batchkarov, Stephen Clark, Daoud Clarke, Roland Davis, Bill Keller, Tamara Polajnar,
Laura Rimell, Mehrnoosh Sadrzadeh, David Sheldrick and Andreas Vlachos. We would
also like to thank the anonymous reviewers for their helpful comments.

32

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

References
Baroni, Marco, Georgiana Dinu, and German

Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting
vs context-predicting semantic vectors. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics.
Baroni, Marco and Alessandro Lenci. 2010.

Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.

Baroni, Marco and Roberto Zamparelli. 2010.
Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions
in semantic space. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing.

Berant, Jonathan and Percy Liang. 2014.
Semantic parsing via paraphrasing. In
Proceedings of ACL.

Blacoe, William and Mirella Lapata. 2012. A

comparison of vector-based
representations for semantic composition.
In Joint Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language Learning.

Bruni, Elia, Nam Khanh Tran, and Marco
Baroni. 2014. Multimodal distributional
semantics. Journal of Artiﬁcial Intelligence
Research, 49:1–47.

Clarke, Daoud. 2007. Context-theoretic

Semantics for Natural Language: an Algebraic
Framework. Ph.D. thesis, University of
Sussex.

Clarke, Daoud. 2012. A context-theoretic

framework for compositionality in
distributional semantics. Computational
Linguistics, 38(1).

Coecke, Bob, Mehrnoosh Sadrzadeh, and

Stephen Clark. 2011. Mathematical
foundations for a compositional
distributed model of meaning. Linguistic
Analysis, 36(1-4):345–384.

Collobert, Ronan and Jason Weston. 2008. A
uniﬁed architecture for natural language
processing: Deep neural networks with
multitask learning. In International
Conference on Machine Learning.

Curran, James. 2004. From Distributional to

Semantic Similarity. Ph.D. thesis, University
of Edinburgh.

Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word cooccurrence probabilities. In 32nd
Annual Meeting of the ACL, pages 272–278.

Dinu, Georgiana, Nghia The Pham, and

Marco Baroni. 2013. General estimation
and evaluation of compositional
distributional semantic models. In

Workshop on Continuous Vector Space Models
and their Compositionality.

Dinu, Georgiana and Stefan Thater. 2012.

Saarland: Vector-based models of semantic
textual similarity. In Proceedings of the First
Joint Conference on Lexical and Computational
Semantics.

Erk, Katrin. 2012. Vector space models of
word meaning and phrase meaning: a
survey. Language and Linguistics Compass,
6(10):635–653.

Erk, Katrin and Sebastian Padó. 2008. A

structured vector space model for word
meaning in context. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing, pages
897–906, Honolulu, Hawaii, October.
Association for Computational Linguistics.

Ferraresi, Adriano, Eros Zanchetta, Marco

Baroni, and Silvia Bernardini. 2008.
Introducing and evaluating ukwac, a very
large web-derived corpus of english. In
Proceedings of LREC.

Finkelstein, Lev, Evgeniy Gabrilovich, Yossi

Matias, Ehud Rivlin, Zach Solan, Gadi
Wolfman, and Eytan Ruppin. 2002. Placing
search in context: The concept revisited.
ACM Transactions on Information Systems,
20(1):116–131.

Garrette, Dan, Katrin Erk, and Raymond

Mooney. 2011. Integrating logical
representations with probabilistic
information using markov logic. In
Proceedings of the Ninth International
Conference on Computational Semantics.
Grefenstette, Edward, Georgiana Dinu,

Yao-Zhong Zhang, Mehrnoosh Sadrzadeh,
and Marco Baroni. 2013. Multi-step
regression learning for compositional
distributional semantics. Proceedings of the
10th International Conference on
Computational Semantics (IWCS 2013).
Guevara, Emiliano. 2010. A Regression

Model of Adjective-Noun
Compositionality in Distributional
Semantics. In Proceedings of the ACL GEMS
Workshop, pages 33–37.

Hermann, Karl Moritz and Phil Blunsom.
2013. The role of syntax in vector space
models of compositional semantics. In
Proceedings of ACL.

Hill, Felix, Roi Reichart, and Anna

Korhonen. 2015. Simlex-999: Evaluating
semantic models with (genuine) similarity
estimation. Computational Linguistics,
41(4):665–695.

Kiela, Douwe, Felix Hill, Anna Korhonen,

and Stephen Clark. 2014. Improving
Multi-Modal Representations Using Image

33

Computational Linguistics

Volume 1, Number 1

Dispersion: Why Less is Sometimes More.
In Proceedings of ACL 2014.

Lambek, J. 1999. Type grammar revisited. In
Alain Lecomte, FranÃ ˇCÂ˘gois Lamarche,
and Guy Perrier, editors, Logical Aspects of
Computational Linguistics, volume 1582 of
Lecture Notes in Computer Science. Springer
Berlin Heidelberg, pages 1–27.

Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 25–32, College Park,
Maryland, USA, June. Association for
Computational Linguistics.

Levy, Omar, Yoav Goldberg, and Ido Dagan.
2015. Improving distributional similarity
with lessons learned from word
embeddings. Transactions of the Association
for Computational Linguistics (TACL).

Levy, Omer and Yoav Goldberg. 2014. Neural

word embeddings as implicit matrix
factorisation. In Proceedings of NIPS.
Lewis, Mike and Mark Steedman. 2013.
Combined distributional and logical
semantics. In Transactions of the Association
for Computational Linguistics (TACL), pages
179–192.

Lin, Dekan. 1998a. An information-theoretic

deﬁnition of similarity. In Proceedings of
International Conference on Machine
Learning.

Lin, Dekang. 1998b. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistings (COLING 1998).

Lund, K. and C. Burgess. 1996. Producing
high-dimensional semantic spaces from
lexical co-occurrence. Behavior Research
Methods, Instrumentation, and Computers,
28:203–208.

McCarthy, Diana and Robert Navigli. 2007.

Semeval-2007 task 10: English lexical
substitution task. In Proceedings of the 4th
International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech
Republic.

Mikolov, Tomas, Kai Chen, Greg Corrado,

and Jeffrey Dean. 2013a. Efﬁcient
estimation of word representations in
vector space. In Proceedings of ICLR
Workshop.

Mikolov, Tomas, Ilya Sutskever, Kai Chen,
Greg Corrado, and Jeffrey Dean. 2013b.
Distributed representations of words and
phrases and their compositionality. In
Proceedings of NIPS.

Mikolov, Tomas, Wen tau Yih, and Geoffrey

Zweig. 2013. Linguistic regularities in
continuous word space representations. In

34

Proceedings of NAACL-HLT.

Mitchell, Jeff and Mirella Lapata. 2008.

Vector-based models of semantic
composition. In Proceedings of ACL-08: HLT,
pages 236–244, Columbus, Ohio, June.
Association for Computational Linguistics.

Mitchell, Jeff and Mirella Lapata. 2010.

Composition in distributional models of
semantics. Cognitive Science,
34(8):1388–1429.

Montague, Richard. 1970. English as a formal

language. In Bruno Visentini, editor,
Linguaggi nella società e nella tecnica. pages
189–223.

Nivre, Joakim. 2004. Incrementality in

deterministic dependency parsing. In
Proceedings of the ACL Workshop on
Incremental Parsing, pages 50–57.

Padó, Sebastian and Mirella Lapata. 2007.

Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161–199.

Pennington, Jeffrey, Richard Socher, and
Christopher D. Manning. 2014. Glove:
Global vectors for word representation. In
Proceedings of EMNLP.

Schütze, Heinrich. 1998. Automatic word

sense discrimination. Computational
Linguistics, 24(1):97–123.

Socher, Richard, Eric Huang, Jeffrey

Pennington, Andrew Ng, and Christopher
Manning. 2011a. Dynamic pooling and
unfolding recursive autoencoders for
paraphrase detection. Advances in Neural
Information Processing Systems, 24.

Socher, Richard, Brody Huval, Christopher D

Manning, and Andrew Y Ng. 2012.
Semantic compositionality through
recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 1201–1211.
Association for Computational Linguistics.

Socher, Richard, Jeffrey Pennington, Eric
Huang, Andrew Ng, and Christopher
Manning. 2011b. Semi-supervised
recursive auto encoders for predicting
sentiment distributions. In Proceedings of
EMNLP, pages 151–161.

Socher, Richard, Alex Perelygin, Jean Wu,
Jason Chuang, Christopher D. Manning,
Andrew Ng, and Christopher Potts. 2013.
Recursive deep models for semantic
compositionality over a sentiment
treebank. In Proceedings of the 2013
Conference on Empirical Methods in Natural
Language Processing, pages 1631–1642,
Seattle, Washington, USA, October.

Weir, Weeds, Refﬁn and Kober

Aligning Packed Dependency Trees

Association for Computational Linguistics.
Steedman, Mark. 2000. The Syntactic Process.

MIT Press.

Wilson, Benjamin. 2015. The unknown perils

of mining wikipedia. https:
//blog.lateral.io/2015/06/
the-unknown-perils-of-mining-wikipedia/.

Zweig, Geoffrey and Chris JC Burges. 2012.

A challenge set for advancing language
modeling. In Proceedings of the
NAACL-HLT 2012 Workshop: Will We Ever
Replace the N-gram Model? On the Future of
Language Modeling for HLT, pages 29–36.

35

Thater, Stefan, Georgiana Dinu, and Manfred

Pinkal. 2009. Ranking paraphrases in
context. In Proceedings of the 2009 Workshop
on Applied Textual Inference, pages 44–47,
Suntec, Singapore, August. Association for
Computational Linguistics.

Thater, Stefan, Hagen Fürstenau, and

Manfred Pinkal. 2010. Contextualizing
semantic representations using
syntactically enriched vector models. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 948–957, Uppsala, Sweden, July.
Association for Computational Linguistics.

Thater, Stefan, Hagen Fürstenau, and

Manfred Pinkal. 2011. Word meaning in
context: A simple and effective vector
model. In Proceedings of 5th International
Joint Conference on Natural Language
Processing (IJCNLP 2011).

Turney, P. D. and P. Pantel. 2010. From
frequency to meaning: Vector space
models of semantics. Journal of Artiﬁcial
Intelligence Research, 37:141–188.

Turney, Peter D. 2012. Domain and function:
A dual-space model of semantic relations
and compositions. Journal of Artiﬁcial
Intelligence Research 44.

Van de Cruys, Tim, Thierry Poibeau, and

Anna Korhonen. 2011. Latent vector
weighting for word meaning in context. In
Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 1012–1022, Edinburgh,
Scotland, UK., July. Association for
Computational Linguistics.

Weeds, Julie. 2003. Measures and Applications

of Lexical Distributional Similarity. Ph.D.
thesis, Department of Informatics,
University of Sussex.

Weeds, Julie and David Weir. 2005.
Co-occurrence retrieval: a ﬂexible
framework for distributional similarity.
Computational Linguistics, 31(4).

Weeds, Julie, David Weir, and Diana

McCarthy. 2004. Characterising measures
of lexical distributional similarity. In
Proceedings of Coling 2004, pages 1015–1021,
Geneva, Switzerland, Aug 23–Aug 27.

Weeds, Julie, David Weir, and Jeremy Refﬁn.

2014. Distributional composition using
higher-order dependency vectors. In
Proceedings of the 2nd Workshop on
Continuous Vector Space Models and their
Composition (EACL 2014), Gothenburg,
Sweden, April.

